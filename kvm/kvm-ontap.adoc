---
sidebar: sidebar 
permalink: kvm/kvm-ontap.html 
keywords: netapp, kvm, libvirt, all-flash, nfs, iscsi, ontap, storage, aff 
summary: 'L"archiviazione condivisa negli host KVM riduce i tempi di migrazione live delle VM e costituisce un obiettivo migliore per i backup e modelli coerenti in tutto l"ambiente.  L"archiviazione ONTAP può soddisfare le esigenze degli ambienti host KVM nonché le richieste di archiviazione di file, blocchi e oggetti guest.' 
---
= Scopri come integrare l'archiviazione ONTAP con gli ambienti di virtualizzazione KVM
:hardbreaks:
:allow-uri-read: 
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ../media/


[role="lead"]
Migliora le prestazioni, la protezione dei dati e l'efficienza operativa integrando l'archiviazione ONTAP con gli ambienti di virtualizzazione KVM tramite Libvirt.  Scopri come le funzionalità di storage di livello aziendale di ONTAP supportano sia i requisiti di storage dell'infrastruttura host KVM sia quelli delle macchine virtuali guest tramite protocolli flessibili NFS, iSCSI e Fibre Channel.

L'archiviazione condivisa negli host KVM riduce i tempi di migrazione live delle VM e costituisce un obiettivo migliore per i backup e modelli coerenti in tutto l'ambiente.  L'archiviazione ONTAP può soddisfare le esigenze degli ambienti host KVM nonché le richieste di archiviazione di file, blocchi e oggetti guest.

Gli host KVM devono disporre di interfacce FC, Ethernet o altre interfacce supportate cablate agli switch e di comunicazione con le interfacce logiche ONTAP .  Controllare sempre https://mysupport.netapp.com/matrix/#welcome["Strumento di matrice di interoperabilità"] per le configurazioni supportate.



== Funzionalità ONTAP di alto livello

*Caratteristiche comuni*

* Cluster scalabile
* Autenticazione sicura e supporto RBAC
* Supporto multi-amministratore Zero Trust
* Multitenancy sicura
* Replica i dati con SnapMirror.
* Copie puntuali con snapshot.
* Cloni efficienti in termini di spazio.
* Funzionalità di efficienza di archiviazione come deduplicazione, compressione, ecc.
* Supporto Trident CSI per Kubernetes
* Chiusura a scatto
* Blocco delle copie snapshot a prova di manomissione
* Supporto per la crittografia
* FabricPool suddivide i dati inattivi in livelli per l'archivio oggetti.
* Integrazione di BlueXP e Data Infrastructure Insights .
* Trasferimento dati scaricato da Microsoft (ODX)


*NAS*

* I volumi FlexGroup sono un contenitore NAS scalabile che garantisce elevate prestazioni insieme a distribuzione del carico e scalabilità.
* FlexCache consente di distribuire i dati a livello globale e fornisce comunque accesso locale in lettura e scrittura ai dati.
* Il supporto multiprotocollo consente di accedere agli stessi dati tramite SMB e NFS.
* NFS nConnect consente più sessioni TCP per connessione TCP, aumentando la produttività della rete.  Ciò aumenta l'utilizzo delle schede di rete ad alta velocità disponibili sui server moderni.
* Il trunking delle sessioni NFS garantisce maggiori velocità di trasferimento dati, elevata disponibilità e tolleranza agli errori.
* pNFS per una connessione ottimizzata del percorso dati.
* La tecnologia SMB multicanale garantisce una maggiore velocità di trasferimento dati, elevata disponibilità e tolleranza agli errori.
* Integrazione con Active Directory/LDAP per i permessi dei file.
* Connessione sicura con NFS su TLS.
* Supporto NFS Kerberos.
* NFS su RDMA.
* Mappatura dei nomi tra identità Windows e Unix.
* Protezione autonoma contro i ransomware.
* Analisi del file system.


*SAN*

* Estendi il cluster su domini di errore con SnapMirror ActiveSync.
* I modelli ASA forniscono multipathing attivo/attivo e failover rapido del percorso.
* Supporto per i protocolli FC, iSCSI, NVMe-oF.
* Supporto per l'autenticazione reciproca iSCSI CHAP.
* Mappa LUN selettiva e Portset.




== Libvirt con archiviazione ONTAP

Libvirt può essere utilizzato per gestire macchine virtuali che sfruttano lo storage NetApp ONTAP per le loro immagini disco e i loro dati.  Questa integrazione consente di sfruttare le funzionalità di archiviazione avanzate di ONTAP, come la protezione dei dati, l'efficienza di archiviazione e l'ottimizzazione delle prestazioni, all'interno dell'ambiente di virtualizzazione basato su Libvirt.  Ecco come Libvirt interagisce con ONTAP e cosa puoi fare:

. Gestione del pool di archiviazione:
+
** Definisci l'archiviazione ONTAP come pool di archiviazione Libvirt: puoi configurare i pool di archiviazione Libvirt in modo che puntino a volumi ONTAP o LUN tramite protocolli come NFS, iSCSI o Fibre Channel.
** Libvirt gestisce i volumi all'interno del pool: una volta definito il pool di archiviazione, Libvirt può gestire la creazione, l'eliminazione, la clonazione e l'acquisizione di snapshot dei volumi all'interno di tale pool, che corrispondono ai LUN o ai file ONTAP .
+
*** Esempio: pool di archiviazione NFS: se gli host Libvirt montano una condivisione NFS da ONTAP, è possibile definire un pool di archiviazione basato su NFS in Libvirt, che elencherà i file nella condivisione come volumi che possono essere utilizzati per i dischi delle VM.




. Archiviazione su disco della macchina virtuale:
+
** Archivia le immagini dei dischi delle macchine virtuali su ONTAP: puoi creare immagini dei dischi delle macchine virtuali (ad esempio, qcow2, raw) all'interno dei pool di archiviazione Libvirt supportati dall'archiviazione ONTAP .
** Sfrutta le funzionalità di archiviazione di ONTAP: quando i dischi delle VM vengono archiviati su volumi ONTAP , traggono automaticamente vantaggio dalla protezione dei dati (Snapshot, SnapMirror, SnapVault), dall'efficienza di archiviazione (deduplicazione, compressione) e dalle funzionalità di prestazioni di ONTAP.


. Protezione dei dati:
+
** Protezione automatizzata dei dati: ONTAP offre una protezione automatizzata dei dati con funzionalità come Snapshot e SnapMirror, che possono proteggere i tuoi dati preziosi replicandoli su altri storage ONTAP , sia in sede, in un sito remoto o nel cloud.
** RPO e RTO: è possibile raggiungere obiettivi di punto di ripristino (RPO) bassi e obiettivi di tempo di ripristino (RTO) rapidi utilizzando le funzionalità di protezione dei dati di ONTAP.
** Sincronizzazione attiva MetroCluster/ SnapMirror : per RPO (Recovery Point Objective) zero automatizzato e disponibilità da sito a sito, è possibile utilizzare ONTAP MetroCluster o SMas, che consente di avere cluster estesi tra siti.


. Prestazioni ed efficienza:
+
** Driver Virtio: utilizza i driver di rete e di dispositivi disco Virtio nelle tue VM guest per migliorare le prestazioni.  Questi driver sono progettati per collaborare con l'hypervisor e offrire vantaggi di paravirtualizzazione.
** Virtio-SCSI: per scalabilità e funzionalità di archiviazione avanzate, utilizzare Virtio-SCSI, che offre la possibilità di connettersi direttamente alle LUN SCSI e di gestire un gran numero di dispositivi.
** Efficienza di archiviazione: le funzionalità di efficienza di archiviazione di ONTAP, come deduplicazione, compressione e compattazione, possono contribuire a ridurre l'ingombro di archiviazione dei dischi delle VM, con conseguente risparmio sui costi.


. Integrazione ONTAP Select :
+
** ONTAP Select su KVM: ONTAP Select, la soluzione di storage software-defined di NetApp, può essere implementata su host KVM, fornendo una piattaforma di storage flessibile e scalabile per le VM basate su Libvirt.
** ONTAP Select Deploy: ONTAP Select Deploy è uno strumento utilizzato per creare e gestire cluster ONTAP Select .  Può essere eseguito come macchina virtuale su KVM o VMware ESXi.




In sostanza, l'utilizzo di Libvirt con ONTAP consente di combinare la flessibilità e la scalabilità della virtualizzazione basata su Libvirt con le funzionalità di gestione dei dati di livello aziendale di ONTAP, fornendo una soluzione solida ed efficiente per il tuo ambiente virtualizzato.



== Pool di archiviazione basato su file (con SMB o NFS)

Per l'archiviazione basata su file sono applicabili pool di archiviazione di tipo dir e netfs.

[cols="20% 10% 10% 10% 10% 10% 10% 10%"]
|===
| Protocollo di archiviazione | direzione | fs | netfs | logico | disco | iscsi | iscsi-diretto | mpath 


| PMI/CIFS | SÌ | NO | SÌ | NO | NO | NO | NO | NO 


| NFS | SÌ | NO | SÌ | NO | NO | NO | NO | NO 
|===
Con netfs, libvirt monterà il file system e le opzioni di montaggio supportate sono limitate.  Con il pool di archiviazione dir, il montaggio del file system deve essere gestito esternamente sull'host. A tale scopo è possibile utilizzare fstab o automounter.  Per utilizzare automounter, è necessario installare il pacchetto autofs.  Autofs è particolarmente utile per montare condivisioni di rete su richiesta, il che può migliorare le prestazioni del sistema e l'utilizzo delle risorse rispetto ai montaggi statici in fstab.  Smonta automaticamente le azioni dopo un periodo di inattività.

In base al protocollo di archiviazione utilizzato, convalidare l'installazione dei pacchetti richiesti sull'host.

[cols="40% 20% 20% 20%"]
|===
| Protocollo di archiviazione | Fedora | Debian | Pac-Man 


| PMI/CIFS | samba-client/cifs-utils | smbclient/cifs-utils | smbclient/cifs-utils 


| NFS | nfs-utils | nfs-comune | nfs-utils 
|===
NFS è una scelta popolare grazie al suo supporto nativo e alle sue prestazioni in Linux, mentre SMB è un'opzione valida per l'integrazione con gli ambienti Microsoft.  Controllare sempre la matrice di supporto prima di utilizzarla in produzione.

In base al protocollo scelto, seguire i passaggi appropriati per creare la condivisione SMB o l'esportazione NFS.https://docs.netapp.com/us-en/ontap-system-manager-classic/smb-config/index.html["Creazione di azioni SMB"] https://docs.netapp.com/us-en/ontap-system-manager-classic/nfs-config/index.html["Creazione dell'esportazione NFS"]

Includere le opzioni di montaggio nel file di configurazione fstab o automounter.  Ad esempio, con autofs, abbiamo incluso la seguente riga in /etc/auto.master per utilizzare la mappatura diretta utilizzando i file auto.kvmfs01 e auto.kvmsmb01

/- /etc/auto.kvmnfs01 --timeout=60 /- /etc/auto.kvmsmb01 --timeout=60 --ghost

e nel file /etc/auto.kvmnfs01, avevamo /mnt/kvmnfs01 -trunkdiscovery,nconnect=4 172.21.35.11,172.21.36.11(100):/kvmnfs01

per smb, in /etc/auto.kvmsmb01, avevamo /mnt/kvmsmb01 -fstype=cifs,credentials=/root/smbpass,multichannel,max_channels=8 ://kvmfs01.sddc.netapp.com/kvmsmb01

Definire il pool di archiviazione utilizzando virsh di tipo pool dir.

[source, shell]
----
virsh pool-define-as --name kvmnfs01 --type dir --target /mnt/kvmnfs01
virsh pool-autostart kvmnfs01
virsh pool-start kvmnfs01
----
Tutti i dischi VM esistenti possono essere elencati utilizzando

[source, shell]
----
virsh vol-list kvmnfs01
----
Per ottimizzare le prestazioni di un pool di archiviazione Libvirt basato su un montaggio NFS, tutte e tre le opzioni Session Trunking, pNFS e l'opzione di montaggio nconnect possono svolgere un ruolo, ma la loro efficacia dipende dalle esigenze specifiche e dall'ambiente.  Ecco una ripartizione per aiutarti a scegliere l'approccio migliore:

. nconnect:
+
** Ideale per: ottimizzazione semplice e diretta del montaggio NFS stesso mediante l'utilizzo di più connessioni TCP.
** Come funziona: l'opzione di montaggio nconnect consente di specificare il numero di connessioni TCP che il client NFS stabilirà con l'endpoint NFS (server).  Ciò può migliorare significativamente la produttività per i carichi di lavoro che traggono vantaggio da più connessioni simultanee.
** Vantaggi:
+
*** Facile da configurare: basta aggiungere nconnect=<numero_di_connessioni> alle opzioni di montaggio NFS.
*** Migliora la produttività: aumenta la "larghezza del pipe" per il traffico NFS.
*** Efficace per vari carichi di lavoro: utile per carichi di lavoro di macchine virtuali di uso generale.


** Limitazioni:
+
*** Supporto client/server: richiede il supporto per nconnect sia sul client (kernel Linux) sia sul server NFS (ad esempio, ONTAP).
*** Saturazione: l'impostazione di un valore nconnect molto elevato potrebbe saturare la linea di rete.
*** Impostazione per montaggio: il valore nconnect viene impostato per il montaggio iniziale e tutti i montaggi successivi sullo stesso server e sulla stessa versione ereditano questo valore.




. Trunking di sessione:
+
** Ideale per: migliorare la produttività e fornire un certo grado di resilienza sfruttando più interfacce di rete (LIF) sul server NFS.
** Come funziona: il trunking di sessione consente ai client NFS di aprire più connessioni a diversi LIF su un server NFS, aggregando di fatto la larghezza di banda di più percorsi di rete.
** Vantaggi:
+
*** Maggiore velocità di trasferimento dati: utilizzando più percorsi di rete.
*** Resilienza: se un percorso di rete non funziona, è comunque possibile utilizzarne altri, anche se le operazioni in corso sul percorso non funzionante potrebbero bloccarsi finché la connessione non viene ristabilita.


** Limitazioni: ancora una singola sessione NFS: sebbene utilizzi più percorsi di rete, non modifica la natura fondamentale della singola sessione dell'NFS tradizionale.
** Complessità della configurazione: richiede la configurazione di gruppi di trunking e LIF sul server ONTAP .  Configurazione di rete: richiede un'infrastruttura di rete adatta a supportare il multipathing.
** Con l'opzione nConnect: l'opzione nConnect verrà applicata solo alla prima interfaccia.  Il resto dell'interfaccia avrà una connessione singola.


. pNFS:
+
** Ideale per: carichi di lavoro ad alte prestazioni e scalabili che possono trarre vantaggio dall'accesso parallelo ai dati e dall'I/O diretto sui dispositivi di archiviazione.
** Come funziona: pNFS separa i metadati dai percorsi dei dati, consentendo ai client di accedere ai dati direttamente dall'archiviazione, bypassando potenzialmente il server NFS per l'accesso ai dati.
** Vantaggi:
+
*** Scalabilità e prestazioni migliorate: per carichi di lavoro specifici come HPC e AI/ML che traggono vantaggio dall'I/O parallelo.
*** Accesso diretto ai dati: riduce la latenza e migliora le prestazioni consentendo ai client di leggere/scrivere i dati direttamente dall'archivio.
*** con l'opzione nConnect: a tutte le connessioni verrà applicato nConnect per massimizzare la larghezza di banda della rete.


** Limitazioni:
+
*** Complessità: pNFS è più complesso da configurare e gestire rispetto ai tradizionali NFS o nconnect.
*** Specifico del carico di lavoro: non tutti i carichi di lavoro traggono significativi benefici da pNFS.
*** Supporto client: richiede il supporto per pNFS sul lato client.






Raccomandazione: * Per pool di archiviazione Libvirt generici su NFS: iniziare con l'opzione di montaggio nconnect.  È relativamente facile da implementare e può garantire un buon incremento delle prestazioni aumentando il numero di connessioni.  * Se hai bisogno di maggiore capacità di elaborazione e resilienza: prendi in considerazione il Session Trunking in aggiunta o al posto di nconnect.  Ciò può essere utile negli ambienti in cui sono presenti più interfacce di rete tra gli host Libvirt e il sistema ONTAP .  * Per carichi di lavoro impegnativi che traggono vantaggio dall'I/O parallelo: se esegui carichi di lavoro come HPC o AI/ML che possono sfruttare l'accesso parallelo ai dati, pNFS potrebbe essere l'opzione migliore.  Tuttavia, bisogna prepararsi a una maggiore complessità di installazione e configurazione.  Testa e monitora sempre le prestazioni NFS con diverse opzioni e impostazioni di montaggio per determinare la configurazione ottimale per il tuo pool di archiviazione Libvirt e il tuo carico di lavoro specifici.



== Pool di archiviazione basato su blocchi (con iSCSI, FC o NVMe-oF)

Un tipo di pool di directory viene spesso utilizzato sopra un file system cluster come OCFS2 o GFS2 su un LUN o namespace condiviso.

Verificare che l'host abbia installati i pacchetti necessari in base al protocollo di archiviazione utilizzato.

[cols="40% 20% 20% 20%"]
|===
| Protocollo di archiviazione | Fedora | Debian | Pac-Man 


| iSCSI | iscsi-initiator-utils,device-mapper-multipath,ocfs2-tools/gfs2-utils | open-iscsi,multipath-tools,ocfs2-tools/gfs2-utils | open-iscsi,multipath-tools,ocfs2-tools/gfs2-utils 


| FC | mappatore-dispositivo-multipercorso,ocfs2-tools/gfs2-utils | strumenti multipath, ocfs2-tools/gfs2-utils | strumenti multipath, ocfs2-tools/gfs2-utils 


| NVMe-oF | nvme-cli,ocfs2-tools/gfs2-utils | nvme-cli,ocfs2-tools/gfs2-utils | nvme-cli,ocfs2-tools/gfs2-utils 
|===
Raccogli host iqn/wwpn/nqn.

[source, shell]
----
# To view host iqn
cat /etc/iscsi/initiatorname.iscsi
# To view wwpn
systool -c fc_host -v
# or if you have ONTAP Linux Host Utility installed
sanlun fcp show adapter -v
# To view nqn
sudo nvme show-hostnqn
----
Fare riferimento alla sezione appropriata per creare il LUN o lo spazio dei nomi.

https://docs.netapp.com/us-en/ontap-system-manager-classic/iscsi-config-rhel/index.html["Creazione LUN per host iSCSI"] https://docs.netapp.com/us-en/ontap-system-manager-classic/fc-config-rhel/index.html["Creazione LUN per host FC"] https://docs.netapp.com/us-en/ontap/san-admin/create-nvme-namespace-subsystem-task.html["Creazione dello spazio dei nomi per gli host NVMe-oF"]

Assicurarsi che i dispositivi FC Zoning o Ethernet siano configurati per comunicare con le interfacce logiche ONTAP .

Per iSCSI,

[source, shell]
----
# Register the target portal
iscsiadm -m discovery -t st -p 172.21.37.14
# Login to all interfaces
iscsiadm -m node -L all
# Ensure iSCSI service is enabled
sudo systemctl enable iscsi.service
# Verify the multipath device info
multipath -ll
# OCFS2 configuration we used.
o2cb add-cluster kvmcl01
o2cb add-node kvm02.sddc.netapp.com
o2cb cluster-status
mkfs.ocfs2 -L vmdata -N 4  --cluster-name=kvmcl01 --cluster-stack=o2cb -F /dev/mapper/3600a098038314c57312b58387638574f
mount -t ocfs2 /dev/mapper/3600a098038314c57312b58387638574f1 /mnt/kvmiscsi01/
mounted.ocfs2 -d
# For libvirt storage pool
virsh pool-define-as --name kvmiscsi01 --type dir --target /mnt/kvmiscsi01
virsh pool-autostart kvmiscsi01
virsh pool-start kvmiscsi01
----
Per NVMe/TCP, abbiamo utilizzato

[source, shell]
----
# Listing the NVMe discovery
cat /etc/nvme/discovery.conf
# Used for extracting default parameters for discovery
#
# Example:
# --transport=<trtype> --traddr=<traddr> --trsvcid=<trsvcid> --host-traddr=<host-traddr> --host-iface=<host-iface>
-t tcp -l 1800 -a 172.21.37.16
-t tcp -l 1800 -a 172.21.37.17
-t tcp -l 1800 -a 172.21.38.19
-t tcp -l 1800 -a 172.21.38.20
# Login to all interfaces
nvme connect-all
nvme list
# Verify the multipath device info
nvme show-topology
# OCFS2 configuration we used.
o2cb add-cluster kvmcl01
o2cb add-node kvm02.sddc.netapp.com
o2cb cluster-status
mkfs.ocfs2 -L vmdata1 -N 4  --cluster-name=kvmcl01 --cluster-stack=o2cb -F /dev/nvme2n1
mount -t ocfs2 /dev/nvme2n1 /mnt/kvmns01/
mounted.ocfs2 -d
# To change label
tunefs.ocfs2 -L tme /dev/nvme2n1
# For libvirt storage pool
virsh pool-define-as --name kvmns01 --type dir --target /mnt/kvmns01
virsh pool-autostart kvmns01
virsh pool-start kvmns01
----
Per FC,

[source, shell]
----
# Verify the multipath device info
multipath -ll
# OCFS2 configuration we used.
o2cb add-cluster kvmcl01
o2cb add-node kvm02.sddc.netapp.com
o2cb cluster-status
mkfs.ocfs2 -L vmdata2 -N 4  --cluster-name=kvmcl01 --cluster-stack=o2cb -F /dev/mapper/3600a098038314c57312b583876385751
mount -t ocfs2 /dev/mapper/3600a098038314c57312b583876385751 /mnt/kvmfc01/
mounted.ocfs2 -d
# For libvirt storage pool
virsh pool-define-as --name kvmfc01 --type dir --target /mnt/kvmfc01
virsh pool-autostart kvmfc01
virsh pool-start kvmfc01
----
NOTA: il montaggio del dispositivo deve essere incluso in /etc/fstab oppure utilizzare i file di mappatura automount.

Libvirt gestisce i dischi virtuali (file) sulla base del file system clusterizzato.  Si basa sul file system clusterizzato (OCFS2 o GFS2) per gestire l'accesso ai blocchi condivisi sottostanti e l'integrità dei dati.  OCFS2 o GFS2 agiscono come un livello di astrazione tra gli host Libvirt e l'archiviazione a blocchi condivisa, fornendo il blocco e il coordinamento necessari per consentire un accesso simultaneo sicuro alle immagini dei dischi virtuali archiviate su tale archiviazione condivisa.
