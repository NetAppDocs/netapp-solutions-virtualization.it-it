---
sidebar: sidebar 
permalink: vmware/vmw-vcf-viwld-supp-nvme.html 
keywords: netapp, vmware, cloud, foundation, vcf, aff, all-flash, nfs, vvol, vvols, array, ontap tools, otv, sddc, iscsi 
summary:  
---
= Aggiungere NVMe su TCP come storage supplementare ai domini dei carichi di lavoro VI
:hardbreaks:
:allow-uri-read: 
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ../media/


[role="lead"]
In questo caso d'uso descriviamo la procedura per utilizzare ONTAP Tools per VMware per configurare NVMe su TCP (NVMe/TCP) come storage supplementare per un dominio di carico di lavoro VMware Cloud Foundation (VCF) Virtual Infrastructure (VI).  Questa procedura riassume la configurazione di una macchina virtuale di archiviazione (SVM) abilitata per NVMe/TCP, la creazione di namespace NVMe, la configurazione della rete host ESXi e la distribuzione di un datastore VMFS.



== Vantaggi di NVMe rispetto a TCP

*Prestazioni elevate:* offre prestazioni eccezionali con bassa latenza e velocità di trasferimento dati elevate.  Ciò è fondamentale per le applicazioni più esigenti e le operazioni sui dati su larga scala.

*Scalabilità:* supporta configurazioni scalabili, consentendo agli amministratori IT di espandere la propria infrastruttura senza problemi man mano che aumentano i requisiti dei dati.

*Economico:* funziona tramite switch Ethernet standard ed è incapsulato all'interno di datagrammi TCP.  Non sono richieste attrezzature speciali per l'implementazione.

Per maggiori informazioni sui vantaggi di NVMe, fare riferimento a https://www.netapp.com/data-storage/nvme/what-is-nvme/["Che cos'è NVME?"]



== Panoramica dello scenario

Questo scenario comprende i seguenti passaggi di alto livello:

* Creare una macchina virtuale di archiviazione (SVM) con interfacce logiche (LIF) per il traffico NVMe/TCP.
* Creare gruppi di porte distribuiti per reti iSCSI sul dominio del carico di lavoro VI.
* Creare adattatori vmkernel per iSCSI sugli host ESXi per il dominio del carico di lavoro VI.
* Aggiungere adattatori NVMe/TCP sugli host ESXi.
* Distribuisci datastore NVMe/TCP.




== Prerequisiti

Questo scenario richiede i seguenti componenti e configurazioni:

* Un sistema di archiviazione ONTAP AFF o ASA con porte dati fisiche su switch Ethernet dedicate al traffico di archiviazione.
* La distribuzione del dominio di gestione VCF è completa e il client vSphere è accessibile.
* In precedenza è stato distribuito un dominio di carico di lavoro VI.


NetApp consiglia progettazioni di rete completamente ridondanti per NVMe/TCP.  Il diagramma seguente illustra un esempio di configurazione ridondante, che garantisce tolleranza agli errori per sistemi di archiviazione, switch, adattatori di rete e sistemi host.  Fare riferimento a NetApplink:https://docs.netapp.com/us-en/ontap/san-config/index.html["Riferimento alla configurazione SAN"] per ulteriori informazioni.

image:vmware-vcf-asa-074.png["Progettazione di rete NVMe-tcp"]

Per il multipathing e il failover su più percorsi, NetApp consiglia di disporre di almeno due LIF per nodo di storage in reti Ethernet separate per tutte le SVM nelle configurazioni NVMe/TCP.

Questa documentazione illustra il processo di creazione di una nuova SVM e di specifica delle informazioni sull'indirizzo IP per creare più LIF per il traffico NVMe/TCP.  Per aggiungere nuovi LIF a un SVM esistente, fare riferimento alink:https://docs.netapp.com/us-en/ontap/networking/create_a_lif.html["Creare un LIF (interfaccia di rete)"] .

Per ulteriori informazioni sulle considerazioni di progettazione NVMe per i sistemi di archiviazione ONTAP , fare riferimento alink:https://docs.netapp.com/us-en/ontap/nvme/support-limitations.html["Configurazione, supporto e limitazioni NVMe"] .



== Fasi di distribuzione

Per creare un datastore VMFS su un dominio di carico di lavoro VCF utilizzando NVMe/TCP, completare i seguenti passaggi.



=== Crea SVM, LIF e namespace NVMe sul sistema di archiviazione ONTAP

Il passaggio seguente viene eseguito in ONTAP System Manager.

.Creare la VM di archiviazione e i LIF
[%collapsible%open]
====
Completare i seguenti passaggi per creare una SVM insieme a più LIF per il traffico NVMe/TCP.

. Da ONTAP System Manager, vai su *VM di archiviazione* nel menu a sinistra e clicca su *+ Aggiungi* per iniziare.
+
image:vmware-vcf-asa-001.png["Fare clic su +Aggiungi per iniziare a creare SVM"]

+
{nbsp}

. Nella procedura guidata *Aggiungi VM di archiviazione*, fornisci un *Nome* per la SVM, seleziona lo *Spazio IP* e quindi, in *Protocollo di accesso*, fai clic sulla scheda *NVMe* e seleziona la casella per *Abilita NVMe/TCP*.
+
image:vmware-vcf-asa-075.png["Aggiungi procedura guidata VM di archiviazione: abilita NVMe/TCP"]

+
{nbsp}

. Nella sezione *Interfaccia di rete* compilare *Indirizzo IP*, *Maschera di sottorete* e *Dominio di broadcast e porta* per il primo LIF.  Per i LIF successivi, la casella di controllo può essere abilitata per utilizzare impostazioni comuni a tutti i LIF rimanenti oppure per utilizzare impostazioni separate.
+

NOTE: Per il multipathing e il failover su più percorsi, NetApp consiglia di disporre di almeno due LIF per nodo di storage in reti Ethernet separate per tutte le SVM nelle configurazioni NVMe/TCP.

+
image:vmware-vcf-asa-076.png["Compila le informazioni di rete per i LIF"]

+
{nbsp}

. Scegliere se abilitare l'account di amministrazione della VM di archiviazione (per ambienti multi-tenancy) e fare clic su *Salva* per creare la SVM.
+
image:vmware-vcf-asa-004.png["Abilita l'account SVM e termina"]



====
.Creare lo spazio dei nomi NVMe
[%collapsible%open]
====
Gli spazi dei nomi NVMe sono analoghi ai LUN per iSCSi o FC.  È necessario creare lo spazio dei nomi NVMe prima di poter distribuire un datastore VMFS da vSphere Client.  Per creare lo spazio dei nomi NVMe, è necessario prima ottenere il nome qualificato NVMe (NQN) da ciascun host ESXi nel cluster.  L'NQN viene utilizzato da ONTAP per fornire il controllo di accesso allo spazio dei nomi.

Per creare uno spazio dei nomi NVMe, completare i seguenti passaggi:

. Aprire una sessione SSH con un host ESXi nel cluster per ottenere il suo NQN.  Utilizzare il seguente comando dalla CLI:
+
[source, cli]
----
esxcli nvme info get
----
+
Dovrebbe essere visualizzato un output simile al seguente:

+
[source, cli]
----
Host NQN: nqn.2014-08.com.netapp.sddc:nvme:vcf-wkld-esx01
----
. Registrare l'NQN per ciascun host ESXi nel cluster
. Da ONTAP System Manager, vai su *NVMe Namespaces* nel menu a sinistra e clicca su *+ Aggiungi* per iniziare.
+
image:vmware-vcf-asa-093.png["Fare clic su +Aggiungi per creare lo spazio dei nomi NVMe"]

+
{nbsp}

. Nella pagina *Aggiungi spazio dei nomi NVMe*, inserisci un prefisso del nome, il numero di spazi dei nomi da creare, la dimensione dello spazio dei nomi e il sistema operativo host che accederà allo spazio dei nomi.  Nella sezione *Host NQN* creare un elenco separato da virgole degli NQN precedentemente raccolti dagli host ESXi che accederanno agli spazi dei nomi.


Fare clic su *Altre opzioni* per configurare elementi aggiuntivi, come il criterio di protezione degli snapshot.  Infine, fare clic su *Salva* per creare lo spazio dei nomi NVMe.

+image:vmware-vcf-asa-093.png["Fare clic su +Aggiungi per creare lo spazio dei nomi NVMe"]

====


=== Configurare adattatori software di rete e NVMe sugli host ESXi

I passaggi seguenti vengono eseguiti sul cluster del dominio del carico di lavoro VI utilizzando il client vSphere.  In questo caso viene utilizzato vCenter Single Sign-On, quindi il client vSphere è comune sia al dominio di gestione che a quello del carico di lavoro.

.Creare gruppi di porte distribuiti per il traffico NVME/TCP
[%collapsible%open]
====
Completare quanto segue per creare un nuovo gruppo di porte distribuito per ogni rete NVMe/TCP:

. Dal client vSphere, accedere a *Inventario > Rete* per il dominio del carico di lavoro.  Passare allo switch distribuito esistente e scegliere l'azione per creare *Nuovo gruppo di porte distribuite...*.
+
image:vmware-vcf-asa-022.png["Scegli di creare un nuovo gruppo di porte"]

+
{nbsp}

. Nella procedura guidata *Nuovo gruppo di porte distribuite*, immettere un nome per il nuovo gruppo di porte e fare clic su *Avanti* per continuare.
. Nella pagina *Configura impostazioni* compila tutte le impostazioni.  Se si utilizzano le VLAN, assicurarsi di fornire l'ID VLAN corretto. Fare clic su *Avanti* per continuare.
+
image:vmware-vcf-asa-023.png["Compila l'ID VLAN"]

+
{nbsp}

. Nella pagina *Pronto per il completamento*, rivedere le modifiche e fare clic su *Fine* per creare il nuovo gruppo di porte distribuite.
. Ripetere questa procedura per creare un gruppo di porte distribuito per la seconda rete NVMe/TCP utilizzata e assicurarsi di aver immesso l'*ID VLAN* corretto.
. Una volta creati entrambi i gruppi di porte, passare al primo gruppo di porte e selezionare l'azione *Modifica impostazioni...*.
+
image:vmware-vcf-asa-077.png["DPG - modifica impostazioni"]

+
{nbsp}

. Nella pagina *Gruppo di porte distribuite - Modifica impostazioni*, vai su *Teaming e failover* nel menu a sinistra e fai clic su *uplink2* per spostarlo in basso a *Uplink non utilizzati*.
+
image:vmware-vcf-asa-078.png["sposta uplink2 in non utilizzato"]

. Ripetere questo passaggio per il secondo gruppo di porte NVMe/TCP.  Questa volta, però, sposta *uplink1* in *Uplink non utilizzati*.
+
image:vmware-vcf-asa-079.png["sposta l'uplink 1 in non utilizzato"]



====
.Creare adattatori VMkernel su ciascun host ESXi
[%collapsible%open]
====
Ripetere questo processo su ciascun host ESXi nel dominio del carico di lavoro.

. Dal client vSphere, passare a uno degli host ESXi nell'inventario del dominio del carico di lavoro.  Dalla scheda *Configura* seleziona *Schede VMkernel* e clicca su *Aggiungi rete...* per iniziare.
+
image:vmware-vcf-asa-030.png["Avvia la procedura guidata di aggiunta della rete"]

+
{nbsp}

. Nella finestra *Seleziona tipo di connessione* seleziona *Scheda di rete VMkernel* e fai clic su *Avanti* per continuare.
+
image:vmware-vcf-asa-008.png["Scegli la scheda di rete VMkernel"]

+
{nbsp}

. Nella pagina *Seleziona dispositivo di destinazione*, seleziona uno dei gruppi di porte distribuite per iSCSI creati in precedenza.
+
image:vmware-vcf-asa-095.png["Scegli il gruppo di porte di destinazione"]

+
{nbsp}

. Nella pagina *Proprietà porta* fare clic sulla casella *NVMe su TCP* e fare clic su *Avanti* per continuare.
+
image:vmware-vcf-asa-096.png["Proprietà della porta VMkernel"]

+
{nbsp}

. Nella pagina *Impostazioni IPv4* compilare *Indirizzo IP*, *Maschera di sottorete* e fornire un nuovo indirizzo IP del gateway (solo se richiesto). Fare clic su *Avanti* per continuare.
+
image:vmware-vcf-asa-097.png["Impostazioni IPv4 di VMkernel"]

+
{nbsp}

. Rivedi le tue selezioni nella pagina *Pronto per il completamento* e fai clic su *Fine* per creare l'adattatore VMkernel.
+
image:vmware-vcf-asa-098.png["Esaminare le selezioni VMkernel"]

+
{nbsp}

. Ripetere questo processo per creare un adattatore VMkernel per la seconda rete iSCSI.


====
.Aggiungi adattatore NVMe su TCP
[%collapsible%open]
====
Ogni host ESXi nel cluster del dominio del carico di lavoro deve disporre di un adattatore software NVMe su TCP installato per ogni rete NVMe/TCP stabilita dedicata al traffico di archiviazione.

Per installare gli adattatori NVMe su TCP e rilevare i controller NVMe, completare i seguenti passaggi:

. Nel client vSphere, passare a uno degli host ESXi nel cluster del dominio del carico di lavoro.  Dalla scheda *Configura* fare clic su *Schede di archiviazione* nel menu e quindi, dal menu a discesa *Aggiungi adattatore software*, selezionare *Aggiungi adattatore NVMe su TCP*.
+
image:vmware-vcf-asa-099.png["Aggiungi adattatore NVMe su TCP"]

+
{nbsp}

. Nella finestra *Aggiungi adattatore software NVMe su TCP*, accedi al menu a discesa *Scheda di rete fisica* e seleziona la scheda di rete fisica corretta su cui abilitare l'adattatore NVMe.
+
image:vmware-vcf-asa-100.png["Seleziona l'adattatore fisico"]

+
{nbsp}

. Ripetere questa procedura per la seconda rete assegnata a NVMe tramite traffico TCP, assegnando l'adattatore fisico corretto.
. Selezionare uno degli adattatori NVMe su TCP appena installati e, nella scheda *Controller*, selezionare *Aggiungi controller*.
+
image:vmware-vcf-asa-101.png["Aggiungi controller"]

+
{nbsp}

. Nella finestra *Aggiungi controller*, seleziona la scheda *Automaticamente* e completa i seguenti passaggi.
+
** Inserire un indirizzo IP per una delle interfacce logiche SVM sulla stessa rete dell'adattatore fisico assegnato a questo adattatore NVMe su TCP.
** Fare clic sul pulsante *Scopri controller*.
** Dall'elenco dei controller rilevati, fare clic sulla casella di controllo per i due controller con indirizzi di rete allineati con questo adattatore NVMe su TCP.
** Fare clic sul pulsante *OK* per aggiungere i controller selezionati.
+
image:vmware-vcf-asa-102.png["Scopri e aggiungi controller"]

+
{nbsp}



. Dopo alcuni secondi dovresti vedere lo spazio dei nomi NVMe apparire nella scheda Dispositivi.
+
image:vmware-vcf-asa-103.png["Spazio dei nomi NVMe elencato sotto i dispositivi"]

+
{nbsp}

. Ripetere questa procedura per creare un adattatore NVMe su TCP per la seconda rete stabilita per il traffico NVMe/TCP.


====
.Distribuisci NVMe su datastore TCP
[%collapsible%open]
====
Per creare un datastore VMFS sullo spazio dei nomi NVMe, completare i seguenti passaggi:

. Nel client vSphere, passare a uno degli host ESXi nel cluster del dominio del carico di lavoro.  Dal menu *Azioni* selezionare *Archiviazione > Nuovo datastore...*.
+
image:vmware-vcf-asa-104.png["Aggiungi adattatore NVMe su TCP"]

+
{nbsp}

. Nella procedura guidata *Nuovo datastore*, selezionare *VMFS* come tipo. Fare clic su *Avanti* per continuare.
. Nella pagina *Selezione nome e dispositivo*, specificare un nome per il datastore e selezionare lo spazio dei nomi NVMe dall'elenco dei dispositivi disponibili.
+
image:vmware-vcf-asa-105.png["Selezione del nome e del dispositivo"]

+
{nbsp}

. Nella pagina *Versione VMFS* selezionare la versione di VMFS per il datastore.
. Nella pagina *Configurazione partizione*, apportare le modifiche desiderate allo schema di partizione predefinito. Fare clic su *Avanti* per continuare.
+
image:vmware-vcf-asa-106.png["Configurazione della partizione NVMe"]

+
{nbsp}

. Nella pagina *Pronto per il completamento*, rivedere il riepilogo e fare clic su *Fine* per creare il datastore.
. Passare al nuovo datastore nell'inventario e fare clic sulla scheda *Host*.  Se configurati correttamente, tutti gli host ESXi nel cluster dovrebbero essere elencati e avere accesso al nuovo datastore.
+
image:vmware-vcf-asa-107.png["Host connessi al datastore"]

+
{nbsp}



====


== Informazioni aggiuntive

Per informazioni sulla configurazione dei sistemi di archiviazione ONTAP fare riferimento alink:https://docs.netapp.com/us-en/ontap["Documentazione ONTAP 9"] centro.

Per informazioni sulla configurazione di VCF fare riferimento alink:https://techdocs.broadcom.com/us/en/vmware-cis/vcf.html["Documentazione di VMware Cloud Foundation"] .
